{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment a test to run an experiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS:\n",
    "# make true for SSL experiments:\n",
    "SSL = False\n",
    "# TEST VALIDATION:\n",
    "# Test 1: Also change cell 13.\n",
    "# log_file = 'test_validation/test1.log'\n",
    "# pcap_file = 'test_validation/test1.pcap'\n",
    "# Test 2: \n",
    "# log_file = 'test_validation/test2.log'\n",
    "# pcap_file = 'test_validation/test2.pcap'\n",
    "# Test 3:\n",
    "# log_file = 'test_validation/test3.log'\n",
    "# pcap_file = 'test_validation/test3.pcap'\n",
    "# Test 4:\n",
    "# log_file = 'test_validation/test4.log'\n",
    "# pcap_file = 'test_validation/test4.pcap'\n",
    "# Test 5:\n",
    "# log_file = 'test_validation/test5.log'\n",
    "# pcap_file = 'test_validation/test5.pcap'\n",
    "# Test 6:\n",
    "# log_file = 'test_validation/test6.log'\n",
    "# pcap_file = 'test_validation/test6.pcap'\n",
    "\n",
    "# NGINX SIEGE:\n",
    "# experiment 1:\n",
    "log_file = 'test/logs-020-nginx-rsyslog-2024-03-20_11-42-25-1.log'\n",
    "pcap_file = 'test/dump-020-nginx-server-2024-03-20_11-42-25-1.pcap'\n",
    "# experiment 2:\n",
    "# log_file = 'test/logs-020-nginx-rsyslog-2024-03-20_11-49-26-1.log'\n",
    "# pcap_file = 'test/dump-020-nginx-server-2024-03-20_11-49-26-1.pcap'\n",
    "# experiment 3:\n",
    "# log_file = 'test/logs-020-nginx-rsyslog-2024-03-20_12-16-49-4.log'\n",
    "# pcap_file = 'test/dump-020-nginx-server-2024-03-20_12-16-49-4.pcap'\n",
    "# experiment 4:\n",
    "# log_file = 'test/logs-020-nginx-rsyslog-2024-03-21_12-48-08-1.log'\n",
    "# pcap_file = 'test/dump-020-nginx-server-2024-03-21_12-48-08-1.pcap'\n",
    "\n",
    "# APACHE WGET:\n",
    "# experiment 1:\n",
    "# log_file = 'test/logs-041-apacheWget-2024-03-24_11-40-06-1.log'\n",
    "# pcap_file = 'test/dump-041-apacheWget-server-2024-03-24_11-40-06-1.pcap'\n",
    "# experiment 2:\n",
    "# log_file = 'test/logs-041-apacheWget-2024-03-24_11-42-37-1.log'\n",
    "# pcap_file = 'test/dump-041-apacheWget-server-2024-03-24_11-42-37-1.pcap'\n",
    "# experiment 3:\n",
    "# log_file = 'test/logs-041-apacheWget-2024-03-24_11-47-35-1.log'\n",
    "# pcap_file = 'test/dump-041-apacheWget-server-2024-03-24_11-47-35-1.pcap'\n",
    "\n",
    "# APACHE SSL:\n",
    "# SSL BEFORE ADJUSTMENTS:\n",
    "# experiment 1:\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-25_12-54-47-1.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-25_12-54-47-1.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-25_12-54-47-1.csv'\n",
    "\n",
    "# experiment 2:\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-25_11-38-04-2.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-25_11-38-04-2.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-25_11-38-04-2.csv'\n",
    "\n",
    "# experiment 3:\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-25_11-38-04-3.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-25_11-38-04-3.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-25_11-38-04-3.csv'\n",
    "\n",
    "# experiment 4:\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-25_11-45-49-1.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-25_11-45-49-1.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-25_11-45-49-1.csv'\n",
    "\n",
    "\n",
    "# SSL AFTER ADJUSTMENTS:\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-24_18-05-23-1.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-24_18-05-23-1.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-24_18-05-23-1.csv'\n",
    "\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-24_18-07-37-1.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-24_18-07-37-1.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-24_18-07-37-1.csv'\n",
    "\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-24_18-09-37-1.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-24_18-09-37-1.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-24_18-09-37-1.csv'\n",
    "\n",
    "# log_file = 'test/logs-042-apachessl-rsyslog-2024-03-24_18-11-42-1.log'\n",
    "# pcap_file = 'test/dump-042-apachessl-rsyslog-server-2024-03-24_18-11-42-1.pcap'\n",
    "# ssl_pcap_csv = 'test/ssl-042-apachessl-rsyslog-2024-03-24_18-11-42-1.csv'\n",
    "\n",
    "\n",
    "# APACHE PATH TRAVERSAL:\n",
    "# experiment 1:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-4.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-4.pcap'\n",
    "# experiment 2:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-13.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-13.pcap'\n",
    "# experiment 3: \n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_17-36-03-1.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_17-36-03-1.pcap'\n",
    "# experiment 4:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-7.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-7.pcap'\n",
    "# experiment 5:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-15.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-15.pcap'\n",
    "# experiment 6:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-18.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-18.pcap'\n",
    "# experiment 7:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-6.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-6.pcap'\n",
    "# experiment 8:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-1.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-1.pcap'\n",
    "# experiment 9:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-3.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-3.pcap'\n",
    "# experiment 10:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_17-46-15-1.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_17-46-15-1.pcap'\n",
    "# experiment 11:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_17-46-15-2.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_17-46-15-2.pcap'\n",
    "# experiment 12:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_17-46-15-3.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_17-46-15-3.pcap'\n",
    "# experiment 13:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-2.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-2.pcap'\n",
    "# experiment 14:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-11.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-11.pcap'\n",
    "# experiment 15:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-14.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-14.pcap'\n",
    "# experiment 16:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-5.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-5.pcap'\n",
    "# experiment 17:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-9.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-9.pcap'\n",
    "# experiment 18:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_18-16-41-1.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_18-16-41-1.pcap'\n",
    "# experiment 19:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-2024-03-20_16-42-34-10.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-20_16-42-34-10.pcap'\n",
    "# experiment 20:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_18-21-39-1.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_18-21-39-1.pcap'\n",
    "# experiment 21:\n",
    "# log_file = 'test/logs-270-path_traversal_apache-rsyslog-2024-03-25_18-21-39-2.log'\n",
    "# pcap_file = 'test/dump-270-path_traversal_apache-server-2024-03-25_18-21-39-2.pcap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log File parsing and field extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv \n",
    "import pandas as pd\n",
    "import re\n",
    "from scapy.all import *\n",
    "\n",
    "logs = []\n",
    "# Open the log file. Each line looks like:\n",
    "# 2024-03-04T11:22:18+00:00 nginx nginx_access: 172.22.0.4 58088 - [04/Mar/2024:11:22:18 +0000] \"GET / HTTP/1.1\" 200 615 \"-\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"-\"\n",
    "with open(log_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Split the line by spaces\n",
    "        parts = line.split(' ')\n",
    "        # Extract rsyslog info parts\n",
    "        rsyslog_time = parts[0]\n",
    "        log_source = parts[1]\n",
    "        log_type = parts[2]\n",
    "\n",
    "        if log_type == 'apache-access' or log_type == 'nginx_access:':\n",
    "            # extract user info\n",
    "            ip = parts[3]\n",
    "            port = parts[4]\n",
    "            user_id = parts[5]\n",
    "            # time is between [ ] \n",
    "            # Extract the content inside square brackets\n",
    "            log_time = re.findall(r'\\[(.*?)\\]', line)[0]\n",
    "            # Extract the content inside quotes\n",
    "            content = re.findall(r'\"(.*?)\"', line)\n",
    "            #print(content)\n",
    "            http_request = content[0]\n",
    "            referrer = content[1]\n",
    "            user_agent = content[2]\n",
    "\n",
    "            # Extract HTTP status code and request method\n",
    "            _, status, bytes_sent = re.findall(r'\\\"(.+?)\\\" (\\d+) (\\d+)', line)[0]\n",
    "\n",
    "            log = {\n",
    "                'rsyslog_time': rsyslog_time,\n",
    "                'log_source': log_source,\n",
    "                'log_type': log_type,\n",
    "                'ip': ip,\n",
    "                'port': port,\n",
    "                'user_id': user_id,\n",
    "                'log_time': log_time,\n",
    "                'http_request': http_request,\n",
    "                'status': status,\n",
    "                'bytes_sent': bytes_sent,\n",
    "                'referrer': referrer,\n",
    "                'user_agent': user_agent\n",
    "            }\n",
    "            # Append the dictionary to the list\n",
    "            logs.append(log)\n",
    "        elif log_type == 'apache-error' or log_type == 'nginx_error:':\n",
    "# format can be: \n",
    "# [Mon Mar 11 13:10:14.881504 2024] [mpm_event:notice] [pid 1:tid 139652787611520] AH00492: caught SIGWINCH, shutting down gracefully\n",
    "# AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.80.3. Set the 'ServerName' directive globally to suppress this message\n",
    "\n",
    "            matches = re.findall(r'\\[(.*?)\\]', line)\n",
    "            if len(matches) > 0:    \n",
    "                log_time = matches[0]\n",
    "                error_source = matches[1]\n",
    "                pid = matches[2]\n",
    "                if len(matches) > 2:\n",
    "                    client = matches[3]\n",
    "                else:\n",
    "                    client = None\n",
    "            else:\n",
    "                log_time = None\n",
    "                error_source = None\n",
    "                pid = None\n",
    "            ah = re.findall(r'AH\\d+', line)[0]\n",
    "            # message is after last ] and before the newline\n",
    "            msg = line.split(']')[-1].strip()\n",
    "            log = {\n",
    "                'rsyslog_time': rsyslog_time,\n",
    "                'log_source': log_source,\n",
    "                'log_type': log_type,\n",
    "                'log_time': log_time,\n",
    "                'error_source': error_source,\n",
    "                'pid': pid,\n",
    "                'client': client,\n",
    "                'msg': msg,\n",
    "                'ah': ah\n",
    "            }\n",
    "            logs.append(log)\n",
    "        else:\n",
    "            logs.append(None)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "# df.to_csv('parsed_logs.csv', index=False, escapechar='\\\\')\n",
    "print(\"Number of logs: \", len(logs))\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pcap file parsing, field extraction and retransmission removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packets = rdpcap(pcap_file)\n",
    "print(\"Number of packets: \", len(packets))\n",
    "if SSL:\n",
    "    # parse from csv generated with tshark:\n",
    "    # entry looks like\n",
    "    # 261\t172.16.239.40\t172.16.239.15\t33330\t443\t453\t1632\tGET\t/\tHTTP/1.1\n",
    "    packets = pd.read_csv(ssl_pcap_csv, delimiter='\\t', names=['index', 'timestamp', 'ip_src', 'ip_dst', 'port_src', 'port_dst', 'tcp_seq', 'tcp_ack', 'http_request_method', 'http_request_path', 'http_version'])\n",
    "    # combine http_request_method, http_request_path, http_version\n",
    "    packets['http_request'] = packets['http_request_method'] + ' ' + packets['http_request_path'] + ' ' + packets['http_version']\n",
    "    packets = packets.drop(columns=['http_request_method', 'http_request_path', 'http_version'])\n",
    "    print(packets.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect request packets\n",
    "def isHTTPRequest(packet):\n",
    "    if packet.haslayer(TCP) and packet.haslayer(Raw):\n",
    "        if packet[TCP].dport == 80 and (b'GET' in packet[Raw].load or b'POST' in packet[Raw].load):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dict_packets = {}\n",
    "if not SSL:\n",
    "    for i, packet in enumerate(packets):\n",
    "        if isHTTPRequest(packet):\n",
    "            timestamp = str(packet.time)\n",
    "            ip_src = packet[IP].src\n",
    "            ip_dst = packet[IP].dst\n",
    "            port_src = packet[TCP].sport\n",
    "            port_dst = packet[TCP].dport\n",
    "            tcp_seq = packet[TCP].seq\n",
    "            # tcp_ack = packet[TCP].ack\n",
    "\n",
    "            http_request = packet[Raw].load.decode('utf-8','replace').split('\\r\\n')[0]\n",
    "    \n",
    "            entry = {\n",
    "                'ip_src': ip_src,\n",
    "                'ip_dst': ip_dst,\n",
    "                'port_src': port_src,\n",
    "                'port_dst': port_dst,\n",
    "                'tcp_seq': tcp_seq,\n",
    "                'http_request': http_request\n",
    "            }\n",
    "\n",
    "            if entry not in dict_packets.values(): # retransmissions removed\n",
    "                dict_packets[i] = entry \n",
    "                \n",
    "    print(\"Number of request packets: \", len(dict_packets))\n",
    "    print(dict_packets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SSL:\n",
    "    for i, packet in packets.iterrows():\n",
    "        index = packet['index']\n",
    "        timestamp = packet['timestamp']\n",
    "        ip_src = packet['ip_src']\n",
    "        ip_dst = packet['ip_dst']\n",
    "        port_src = packet['port_src']\n",
    "        port_dst = packet['port_dst']\n",
    "        tcp_seq = packet['tcp_seq']\n",
    "        tcp_ack = packet['tcp_ack']\n",
    "        http_request = packet['http_request']\n",
    "\n",
    "        entry = {\n",
    "            'ip_src': ip_src,\n",
    "            'ip_dst': ip_dst,\n",
    "            'port_src': port_src,\n",
    "            'port_dst': port_dst,\n",
    "            'tcp_seq': tcp_seq,\n",
    "            'tcp_ack': tcp_ack,\n",
    "            'http_request': http_request\n",
    "        }\n",
    "        if entry not in dict_packets.values(): # retransmissions removed\n",
    "            dict_packets[index] = entry\n",
    "\n",
    "    print(len(dict_packets))\n",
    "    print(dict_packets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all packets matching with log fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = {}\n",
    "\n",
    "for i, log in enumerate(logs):\n",
    "    if log['log_type'] == 'apache-access' or log['log_type'] == 'nginx_access:':\n",
    "        log_ip = log['ip']\n",
    "        log_port = int(log['port'])\n",
    "        log_request = log['http_request']\n",
    "        # print(log_ip, log_port, log_request)\n",
    "        matching_packets = []\n",
    "        for key, packet in dict_packets.items():\n",
    "            packet_ip = packet['ip_src']\n",
    "            packet_port = packet['port_src']\n",
    "            packet_request = packet['http_request']\n",
    "\n",
    "            if log_ip == packet_ip and log_port == packet_port and log_request == packet_request:\n",
    "                matching_packets.append(key)\n",
    "            \n",
    "        if len(matching_packets) > 0:\n",
    "            fusion[i] = matching_packets\n",
    "        else: \n",
    "            print('No matching packet for log', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion contains the initial matching of logs and packets, may contain one-to-many matches\n",
    "print(fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple match alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion2 = {}\n",
    "similar_entries = []\n",
    "count = 0\n",
    "for key, value in fusion.items():\n",
    "    if len(value) > 1:\n",
    "        # find all logs with the same entries as value\n",
    "        same_entries = [k for k, v in fusion.items() if v == value]\n",
    "        count += 1\n",
    "        print('Similar entries found: ', key, value, same_entries)\n",
    "        #if len(value) > len(same_entries): # HANDLED IN TESTS INSTEAD\n",
    "        #    raise Exception(f'Retransmission found for log {key}')\n",
    "        similar_entries.append((same_entries, value))\n",
    "        for log_id, packet_id in zip(same_entries, value):\n",
    "            fusion2[log_id] = packet_id\n",
    "    else:\n",
    "        fusion2[key] = value[0]\n",
    "\n",
    "print(\"Similar logs found: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion2 contains the final matching of logs and packets, one-to-one matches\n",
    "print(fusion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_packets.keys():\n",
    "    if key not in fusion2.values():\n",
    "        print('No matching log for packet', key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Log error ID Matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle error logs:\n",
    "for i, log in enumerate(logs):\n",
    "    if log['log_type'] == 'apache-error' or log['log_type'] == 'nginx_error:':\n",
    "        # invalid URI path error:\n",
    "        # [Wed Mar 20 16:16:57.438276 2024] [core:error] [pid 8:tid 140089493489408] [client 172.16.27.3:57178] AH10244: invalid URI path (//icons/.%2e/%2e%2e/%2e%2e/%2e%2e/etc/passwd)\n",
    "        if log['ah'] == 'AH10244':\n",
    "            client_ip = log['client'].split(' ')[1].split(':')[0]\n",
    "            client_port = log['client'].split(' ')[1].split(':')[1]\n",
    "            invalid_uri = log['msg'].split(' ')[-1][1:-1]\n",
    "            matching_logs = []\n",
    "            for j, log2 in enumerate(logs):\n",
    "                if log2['log_type'] == 'apache-access' or log2['log_type'] == 'nginx_access:':\n",
    "                    # check if invalid_uri is a substring of http_request\n",
    "                    if client_ip == log2['ip'] and client_port == log2['port'] and invalid_uri in log2['http_request']:\n",
    "                        matching_logs.append(j)\n",
    "\n",
    "        # add other error types\n",
    "        elif log['ah'] == 'AH00132':\n",
    "            client_ip = log['client'].split(' ')[1].split(':')[0]\n",
    "            client_port = log['client'].split(' ')[1].split(':')[1]\n",
    "            file = log['msg'].split(' ')[-1]\n",
    "            matching_logs = []\n",
    "            for j, log2 in enumerate(logs):\n",
    "                if log2['log_type'] == 'apache-access' or log2['log_type'] == 'nginx_access:':\n",
    "                    # check if invalid_uri is a substring of http_request\n",
    "                    if client_ip == log2['ip'] and client_port == log2['port'] and file in log2['http_request']:\n",
    "                        matching_logs.append(j)\n",
    "        else:\n",
    "            print('Error log not handled: ', log)\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if len(matching_logs) == 0:\n",
    "            raise Exception('No matching logs found for error log ', i)\n",
    "        elif len(matching_logs) == 1:\n",
    "            fusion2[i] = fusion2[matching_logs[0]]\n",
    "        else:\n",
    "            raise Exception('Multiple matching logs found for error log ', i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fusion2)\n",
    "# to make validation test 1 fail, swap first 2 matches: \n",
    "# fusion2[0] = 41\n",
    "# fusion2[1] = 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS:\n",
    "def error_log_timestamp_to_unix(string):\n",
    "    date_format_error_log = \"%a %b %d %H:%M:%S.%f %Y\"\n",
    "    date_object = datetime.strptime(string, date_format_error_log)\n",
    "    return date_object.timestamp()\n",
    "\n",
    "def access_log_timestamp_to_unix(string):\n",
    "    date_format_access_log = \"%d/%b/%Y:%H:%M:%S %z\"\n",
    "    date_object = datetime.strptime(string, date_format_access_log)\n",
    "    return date_object.timestamp()\n",
    "\n",
    "\n",
    "# test if attrubutes of log and packet match\n",
    "def test_log_packet_attributes(log_id, packet_id):\n",
    "    log = logs[log_id]\n",
    "    packet = dict_packets[packet_id]\n",
    "    if log['log_type'] == 'apache-access' or log['log_type'] == 'nginx_access:':\n",
    "        log_ip = log['ip']\n",
    "        log_port = int(log['port'])\n",
    "        log_request = log['http_request']\n",
    "\n",
    "        packet_ip = packet['ip_src']\n",
    "        packet_port = packet['port_src']\n",
    "        packet_request = packet['http_request']\n",
    "\n",
    "        if log_ip == packet_ip and log_port == packet_port and log_request == packet_request:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# test if logs with multiple matches, also have the same number of matching logs     \n",
    "def test_multiple_match_sizes(similar_entries):\n",
    "    for similar_entry in similar_entries:\n",
    "        same_entries, value = similar_entry\n",
    "        if len(same_entries) != len(value):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# check that similar entries were sorted by time and matched in the correct order\n",
    "def test_multiple_match_allocated_order(similar_entries):\n",
    "    for similar_entry in similar_entries:\n",
    "        log_entries, packet_entries = similar_entry\n",
    "        for log_id, packet_id in zip(log_entries, packet_entries):\n",
    "            if fusion2[log_id] != packet_id:\n",
    "                return False\n",
    "        # check that log entries and pcap entries are ascending with time\n",
    "        log_times = [logs[i]['rsyslog_time'] for i in log_entries]\n",
    "        packet_times = [packets[i].time for i in packet_entries]\n",
    "        if log_times != sorted(log_times):\n",
    "            return False\n",
    "        if packet_times != sorted(packet_times):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# check that timestamps are within 1 second of each other\n",
    "def test_timestamps(logs, packets, fusion):\n",
    "    for log_id, packet_id in fusion.items():\n",
    "        log = logs[log_id]\n",
    "        packet = packets[packet_id]\n",
    "        packet_time = packet.time\n",
    "        if log['log_type'] == 'apache-access' or log['log_type'] == 'nginx_access:':\n",
    "            log_time = access_log_timestamp_to_unix(log['log_time'])\n",
    "        elif log['log_type'] == 'apache-error' or log['log_type'] == 'nginx_error:':\n",
    "            log_time = error_log_timestamp_to_unix(log['log_time'])\n",
    "        else:\n",
    "            raise Exception('Unknown log type: ', log['log_type'])\n",
    "        if (abs(log_time - packet_time) > 1):\n",
    "            print('Timestamp gaps too big: log ', log_id, ', packet ', packet_id)\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# check that each packet is matched to only one access log. Ignore error logs:\n",
    "def test_one_to_one(fusion, logs):\n",
    "    if len(dict_packets) != len(fusion):\n",
    "        print('Different number of packets and logs: ', len(dict_packets), len(fusion))\n",
    "        return False\n",
    "    error_log_keys = [i for i, log in enumerate(logs) if log['log_type'] == 'apache-error' or log['log_type'] == 'nginx_error:']\n",
    "\n",
    "    for log_id, packet_id in fusion.items():\n",
    "        count = 0\n",
    "        for log_id2, packet_id2 in fusion.items():\n",
    "            if packet_id == packet_id2 and log_id not in error_log_keys and log_id2 not in error_log_keys:\n",
    "                count += 1\n",
    "        if count > 1:\n",
    "            print('Packet matched to multiple logs: ', packet_id)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# check that all logs have a matching packet\n",
    "def test_all_logs_matched(fusion, logs):\n",
    "    all_found = True\n",
    "    for i, log in enumerate(logs):\n",
    "        if i not in fusion.keys():\n",
    "            print('No matching packet for log: ', i)\n",
    "            all_found = False\n",
    "    return all_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = False\n",
    "for log_id, packet_id in fusion2.items():\n",
    "    if not test_log_packet_attributes(log_id, packet_id):\n",
    "        failed = True\n",
    "        print(f'Log {log_id} does not match packet {packet_id}')\n",
    "if failed:\n",
    "    print('Test 1 failed')\n",
    "else:\n",
    "    print('Test 1 passed')\n",
    "\n",
    "if not test_multiple_match_sizes(similar_entries):\n",
    "    print('Similar entries are not of the same size')\n",
    "else: \n",
    "    print('Test 2 passed')\n",
    "\n",
    "if not test_multiple_match_allocated_order(similar_entries):\n",
    "    print('Similar entries matched incorrectly')\n",
    "else:\n",
    "    print('Test 3 passed')\n",
    "\n",
    "\n",
    "if SSL:\n",
    "    packets = rdpcap(pcap_file)\n",
    "\n",
    "if not test_timestamps(logs, packets, fusion2):\n",
    "    print('Timestamps do not match')\n",
    "else:\n",
    "    print('Test 4 passed')\n",
    "\n",
    "if not test_one_to_one(fusion2, logs):\n",
    "    print('One to one matching test failed')\n",
    "else:\n",
    "    print('Test 5 passed')\n",
    "\n",
    "if not test_all_logs_matched(fusion2, logs):\n",
    "    print('Not all logs have a matching packet')\n",
    "else:\n",
    "    print('Test 6 passed')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
